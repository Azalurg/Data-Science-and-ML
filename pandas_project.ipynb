{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0686c98e-72e8-46e1-9915-a2eaecb54294",
   "metadata": {},
   "source": [
    "# Pandas tutorial\n",
    "\n",
    "Dataset characteristic:\n",
    "\n",
    "- every folder contain: addresses.csv, addresses_people.csv, people.csv, people_publications.csv, publications.csv.\n",
    "- every file contain few columns, named in first row\n",
    "- every file not with many-to-many relations, contain temp_id column\n",
    "\n",
    "Project:\n",
    "\n",
    "- merge all files into single DataFrame\n",
    "- change current temp_ids into new unique ids\n",
    "- eliminate duplicates (eg. consider address with x percentage of similarity as one)\n",
    "- add column “town” for people.csv data, extracted from “addresses” column in addresses.csv\n",
    "- get missing lat/lng data for every town/address (eg. through google api)\n",
    "- save DataFrame to single csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60c28df-d94c-4ef3-850c-af6274be3620",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84c2760b-9a09-4e55-ab24-7ea552f93a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_percentage = 20\n",
    "\n",
    "data_directory=\"./data\"\n",
    "data_sub_folders = 134\n",
    "data_load_step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5e1edb",
   "metadata": {},
   "source": [
    "## Functions & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "837f5a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def move_head(arr, index: int):\n",
    "    arr = arr = [arr[index]] + arr[:index] + arr[index + 1:]\n",
    "\n",
    "def data_filter(name: str, address: str):\n",
    "    data = name.split(\",\") + address.split(\",\")\n",
    "    filtered_data = []\n",
    "    for word in data:\n",
    "        cleaned_word = word.strip().lower()\n",
    "        cleaned_word = re.sub(r'[^a-zA-Z\\s]', ' ', cleaned_word)\n",
    "        cleaned_word = re.sub(r'\\s+', ' ', cleaned_word)\n",
    "        if len(cleaned_word) > 2:\n",
    "            filtered_data.append(cleaned_word.strip())\n",
    "    return \",\".join(filtered_data)\n",
    "            \n",
    "def similarity(data1: str, data2: str):\n",
    "    arr1, arr2 = data1.split(\",\"), data2.split(\",\")\n",
    "    counter = 0\n",
    "    total = max(len(arr1), len(arr2))\n",
    "    for str1 in arr1:\n",
    "        for str2 in arr2:\n",
    "            if str1 in str2 or str2 in str1:\n",
    "                counter += 1\n",
    "    return counter / total * 100\n",
    "\n",
    "def make_id_unique(df, value, column=\"temp_id\"):\n",
    "    df[column] = df[column].apply(lambda id: str(value) + \"_\" + str(id))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d71be7a",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9ede55e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address_uuid</th>\n",
       "      <th>person_uuid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_2</td>\n",
       "      <td>0_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0_3</td>\n",
       "      <td>0_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_4</td>\n",
       "      <td>0_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0_4</td>\n",
       "      <td>0_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0_5</td>\n",
       "      <td>0_4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  address_uuid person_uuid\n",
       "0          0_2         0_1\n",
       "1          0_3         0_2\n",
       "2          0_4         0_2\n",
       "3          0_4         0_3\n",
       "4          0_5         0_4"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = {\"addresses\": [], \"addresses_people\": [], \"people\": [], \"people_publications\": [], \"publications\": []}\n",
    "\n",
    "for i in range(0, data_sub_folders, data_load_step):\n",
    "    data_files[\"addresses\"].append(make_id_unique(pd.read_csv(os.path.join(data_directory, f\"{i}/ADDRESSES.csv\")), i))\n",
    "    data_files[\"addresses_people\"].append(make_id_unique(make_id_unique(pd.read_csv(os.path.join(data_directory, f\"{i}/ADDRESSES_PEOPLE.csv\")), i, \"address_uuid\"), i, \"person_uuid\"))\n",
    "    data_files[\"people\"].append(make_id_unique(pd.read_csv(os.path.join(data_directory, f\"{i}/PEOPLE.csv\")), i))\n",
    "    data_files[\"people_publications\"].append(make_id_unique(make_id_unique(pd.read_csv(os.path.join(data_directory, f\"{i}/PEOPLE_PUBLICATIONS.csv\")), i, \"person_uuid\"), i, \"publication_uuid\"))\n",
    "    data_files[\"publications\"].append(make_id_unique(pd.read_csv(os.path.join(data_directory, f\"{i}/PUBLICATIONS.csv\")), i))\n",
    "\n",
    "address_df = pd.concat(data_files[\"addresses\"], ignore_index=True)\n",
    "addresses_people_df = pd.concat(data_files[\"addresses_people\"], ignore_index=True)\n",
    "people_df = pd.concat(data_files[\"people\"], ignore_index=True)\n",
    "people_publications_df = pd.concat(data_files[\"people_publications\"], ignore_index=True)\n",
    "publications_df = pd.concat(data_files[\"publications\"], ignore_index=True)\n",
    "\n",
    "addresses_people_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9e0a34",
   "metadata": {},
   "source": [
    "## Prepare good addresses indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bef5beeb-67f0-46ce-9c4a-521218cc826d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'temp_uuid'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Intern/06. Data Science/venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'temp_uuid'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:14\u001b[0m\n",
      "File \u001b[0;32m~/Intern/06. Data Science/venv/lib/python3.10/site-packages/pandas/core/series.py:1112\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/Intern/06. Data Science/venv/lib/python3.10/site-packages/pandas/core/series.py:1228\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1228\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/Intern/06. Data Science/venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'temp_uuid'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "address_df[\"full_address\"] = address_df.apply(lambda x: data_filter(str(x[\"name\"]), str(x[\"address\"])), axis=1)\n",
    "\n",
    "good_address_list = []  # (address[\"full_address\"], address[\"temp_uuid\"], [])\n",
    "\n",
    "for address_index, address in address_df.iterrows():\n",
    "    flag = True\n",
    "    for good_address_index, ga in enumerate(good_address_list):\n",
    "        if similarity(ga[0], address[\"full_address\"]) > similarity_percentage:\n",
    "            flag = False\n",
    "            ga[2].append(address[\"temp_uuid\"])\n",
    "            move_head(good_address_list, good_address_index)\n",
    "            break\n",
    "    if flag:    \n",
    "        good_address_list.append((address[\"full_address\"], address[\"temp_uuid\"], []))\n",
    "\n",
    "address_df = address_df.drop([\"full_address\"], axis=1)\n",
    "\n",
    "print(f\"{len(good_address_list)} good addresses found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bbae6a",
   "metadata": {},
   "source": [
    "## Update ADDRESSES_PEOPLE relation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a266419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "20\n",
      "30\n",
      "44\n",
      "59\n",
      "65\n",
      "82\n",
      "118\n",
      "224\n",
      "352\n",
      "360\n",
      "457\n",
      "462\n",
      "492\n",
      "552\n",
      "554\n",
      "715\n",
      "735\n",
      "905\n",
      "937\n",
      "959\n",
      "1033\n",
      "1224\n",
      "1227\n",
      "1467\n",
      "1646\n",
      "1875\n",
      "1979\n",
      "2068\n",
      "2197\n",
      "2198\n",
      "2250\n",
      "2305\n",
      "2319\n",
      "2944\n",
      "3165\n",
      "3289\n",
      "3532\n",
      "3659\n",
      "4402\n",
      "4480\n",
      "6220\n",
      "6511\n",
      "7163\n",
      "7280\n",
      "7608\n",
      "7953\n",
      "8251\n",
      "8314\n",
      "8460\n",
      "8773\n",
      "9364\n",
      "9590\n",
      "10151\n",
      "10394\n",
      "10695\n",
      "11112\n",
      "11986\n",
      "12122\n",
      "12426\n",
      "12656\n",
      "14007\n",
      "14475\n",
      "15182\n",
      "15286\n",
      "15500\n",
      "16193\n",
      "16655\n",
      "17594\n",
      "17856\n",
      "18094\n",
      "18271\n",
      "18470\n",
      "19612\n",
      "20534\n",
      "20680\n",
      "20726\n",
      "21031\n",
      "21071\n",
      "21681\n",
      "21709\n",
      "22341\n",
      "24264\n",
      "26195\n",
      "26644\n",
      "26916\n",
      "28088\n",
      "28118\n",
      "28263\n",
      "28754\n",
      "29347\n",
      "29838\n",
      "29846\n",
      "30297\n",
      "32032\n",
      "33644\n",
      "33940\n",
      "34625\n",
      "34951\n",
      "35697\n",
      "36223\n",
      "39539\n",
      "40279\n",
      "40692\n",
      "41002\n",
      "42307\n",
      "43257\n",
      "43361\n",
      "43744\n",
      "43783\n",
      "44377\n",
      "44614\n",
      "46155\n",
      "46437\n",
      "47242\n",
      "47347\n",
      "47931\n",
      "48809\n",
      "49394\n",
      "49909\n",
      "50234\n",
      "51664\n",
      "52055\n",
      "52145\n",
      "52391\n",
      "55435\n",
      "56134\n",
      "57189\n",
      "57628\n",
      "58375\n",
      "59828\n",
      "61249\n",
      "65275\n",
      "66080\n",
      "66915\n",
      "68228\n",
      "68767\n",
      "69240\n",
      "70933\n",
      "73047\n",
      "76726\n",
      "76997\n",
      "77175\n",
      "78458\n",
      "78595\n",
      "80038\n",
      "81685\n",
      "82785\n",
      "83886\n",
      "85936\n",
      "86779\n",
      "87078\n",
      "88047\n",
      "91325\n",
      "91675\n",
      "91716\n",
      "92126\n",
      "92529\n",
      "94207\n",
      "98150\n",
      "99486\n",
      "101243\n",
      "101363\n",
      "102254\n",
      "103766\n",
      "105767\n",
      "108252\n",
      "109258\n",
      "109412\n",
      "110966\n",
      "111738\n",
      "117178\n",
      "117537\n",
      "117936\n",
      "119060\n",
      "119680\n",
      "119877\n",
      "121196\n",
      "121567\n",
      "121682\n",
      "123138\n",
      "126237\n",
      "129887\n",
      "130147\n",
      "133285\n",
      "134014\n",
      "134431\n",
      "135391\n",
      "139579\n",
      "139696\n",
      "141425\n",
      "142460\n",
      "143990\n",
      "144928\n",
      "145870\n",
      "147268\n",
      "147425\n",
      "155542\n",
      "156263\n",
      "159929\n",
      "160139\n",
      "165115\n",
      "167712\n",
      "168891\n",
      "169569\n",
      "169806\n",
      "172436\n",
      "174424\n",
      "176195\n",
      "184585\n",
      "186454\n",
      "188554\n",
      "189964\n",
      "190333\n",
      "193257\n",
      "195105\n"
     ]
    }
   ],
   "source": [
    "for address_str, good_address_index, to_update_address_list in good_address_list:\n",
    "    for to_replace_address_index in to_update_address_list:\n",
    "        addresses_people_df.loc[to_replace_address_index, \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d6c8e3",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94732f1b-6a7b-4223-a17a-5d0ff304b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for it in os.scandir(\"./data\"):\n",
    "    print(it)\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(\"./data/\"):\n",
    "    if dirnames:\n",
    "        continue\n",
    "    addresses = pd.read_table(os.path.join(dirpath, \"ADDRESSES.csv\"), sep=',')\n",
    "    addresses_people = pd.read_table(os.path.join(dirpath, \"ADDRESSES_PEOPLE.csv\"), sep=',')\n",
    "    people = pd.read_table(os.path.join(dirpath, \"PEOPLE.csv\"), sep=\",\")\n",
    "    people_publications = pd.read_table(os.path.join(dirpath, \"PEOPLE_PUBLICATIONS.csv\"), sep=\",\")\n",
    "    publications = pd.read_table(os.path.join(dirpath, \"PUBLICATIONS.csv\"), sep=\",\")\n",
    "\n",
    "    addresses.rename(columns={\"temp_id\": \"address_uuid\"}, inplace=True)\n",
    "    people.rename(columns={\"temp_id\": \"person_uuid\"}, inplace=True)\n",
    "    publications.rename(columns={\"temp_id\": \"publication_uuid\"}, inplace=True)\n",
    "    \n",
    "    merged_1 = pd.merge(addresses, addresses_people, on='address_uuid', how=\"inner\")\n",
    "    merged_2 = pd.merge(merged_1, people, on=\"person_uuid\", how=\"inner\")\n",
    "    merged_3 = pd.merge(merged_2, people_publications, on=\"person_uuid\", how=\"inner\")\n",
    "    data = pd.merge(merged_3, publications, on=\"publication_uuid\", how=\"inner\")\n",
    "\n",
    "    dfs.append(data)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43c1184-133c-4495-9cba-99a6b654fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id'] = df['address_uuid'].astype(str) + \"_\" + df['person_uuid'].astype(str) + \"_\" + df['publication_uuid'].astype(str)\n",
    "df = df[['id'] + [col for col in df.columns if col != 'id']]\n",
    "df.drop(columns=['address_uuid', 'person_uuid', \"publication_uuid\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f8b3cc-1534-475b-b325-b3fa33b0571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('address').size().reset_index(name='count').sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4243df-8e11-4fd2-8e8b-666e7fe88149",
   "metadata": {},
   "outputs": [],
   "source": [
    "visited = set()\n",
    "c = 0\n",
    "i = 0\n",
    "for index, row in df.iterrows():\n",
    "    i+=1\n",
    "    current_address = row['address']\n",
    "    current_name = row['name']\n",
    "    v = str(current_address) + \", \" + str(current_name)\n",
    "    if v in visited:\n",
    "        c += 1\n",
    "        continue\n",
    "    visited.add(v)\n",
    "\n",
    "c, i, c-i\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6735721-09a5-450e-adb3-6ce4f6e6b23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.at[1, \"firstname\"] = \"Paweł\"\n",
    "df.iloc[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
